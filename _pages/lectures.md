---
author: phwl
comments: false
date: 2017-10-21 09:16:08+00:00
# layout: page
# link: http://phwl.org/lectures/
permalink: lectures
slug: lectures
title: lectures
wordpress_id: 3312
---

# 2019





 	
  * 10th May 2019, [Efficient FPGA implementations of Machine Learning Algorithms](/assets/images/2019/05/efficientML19.pdf), San Jose State University


FPGA implementations of machine learning algorithms have been shown to be extremely efficient when the problem fits entirely on the FPGA but it remains a challenge to scale to problems of interest to industry. In this talk, our recent research on how to increase the capacity of existing approaches will be described.

In the first part of the talk we will describe an implementation of the Fastfood algorithm for scaling up online kernel methods. By utilizing the theory of random projections, problems with 1000x larger input dimension and dictionary size can be solved. A systolic implementation that operates at 500 MHz clock frequency was achieved. The next part of the talk describes how the resource requirements  in convolutional neural networks can be reduced using symmetric quantisation (SYQ), which achieves state of the art accuracy for binary and ternary weights. Finally, our work on new DSP-block architectures within FPGAs, optimised for energy-efficient embedded machine learning will be presented.


# 2018





 	
  * 7th December 2018, [Large Scale FPGA Implementations of Machine Learning Algorithms](/assets/images/2018/12/largescaleML18.pdf), Tokyo Institute of Technology


_FPGA implementations of machine learning algorithms have been shown to be extremely efficient when the problem fits entirely on the FPGA but it remains a challenge to scale to problems of interest to industry. In this talk, our recent research on how to increase the capacity of existing approaches will be described._

_In the first part of the talk we will describe an implementation of the Fastfood algorithm for scaling up online kernel methods. By utilizing the theory of random projections, problems with 1000x larger input dimension and dictionary size can be solved. A systolic implementation that operates at 500 MHz clock frequency was achieved. The next part of the talk describes how the resource requirements  in convolutional neural networks can be reduced using symmetric quantisation (SYQ), which achieves state of the art accuracy for binary and ternary weights. Finally, our work on an open-source matrix multiplication library on the Xeon+FPGA platform is described. This allows a number of different precisions down to binary, and additional support for deep learning applications is provided. Performance approaching the maximum of 14 Xeon cores and an Arria 10 FPGA was achieved._



 	
  * 26 November 2018, [Fixed-point FPGA implementations of Machine Learning Algorithms](/assets/images/2018/11/fixedml-imperial18.pdf), Department of Computing, Imperial College


_Abstract: FPGA implementations of machine learning algorithms have been shown to be extremely efficient, particularly when fixed-point arithmetic can be utilised to simplify the datapath of a hardware implementation. In this talk, our recent research on how accuracy and performance of fixed-point implementations of neural networks can be improved will be presented._

_In the first part of the talk we describe how the resource requirements  in convolutional neural networks can be reduced using symmetric quantisation (SYQ), which achieves state of the art accuracy for binary and ternary weights. Next, a variant of the serial-parallel modified radix-4 Booth multiplier that adds only the non-zero Booth encodings and skips over the zero operations, achieving a 1.4x Area-Time improvement over the standard parallel Booth multiplier is presented. Finally, we describe a system implementation of a long short-term memory (LSTM) based spectral prediction accelerator for physical layer radio frequency applications, with latency below 5 microseconds._



 	
  * 29 October 2018, [Long short-term memory for radio frequency spectral prediction and its real-time FPGA implementation](/assets/images/2018/10/lstmslides-milcom18.pdf), MILCOM Los Angeles

 	
  * 26 September 2018, [Teaching and Research at the University of Sydney](/assets/images/2018/09/TeachingandResearchatUSyd-HIT18.pdf), Harbin Institute of Technology

 	
  * 26 September 2018, [Reconfigurable Computing](/harbin-intitute-of-technology-reconfigurable-computing-course-2018/), Harbin Institute of Technology Short Graduate Course

 	
  * 25 September 2018, [Discover your future in engineering and technology](/assets/images/2018/09/DLT-presentation-60-min.pdf), D & LT Consultants, Harbin

 	
  * 24 July 2018, [Quantisation](/assets/images/2018/11/quantisation-papaa18.pdf), [Performance-Aware Programming with Application Accelerators (PAPAA)](http://cscpapaa.eee.hku.hk/), Hong Kong




# 2017





 	
  * 30 November 2017,  [FPGA-based Implementations of Machine Learning Algorithms and the EPIC Approach](/assets/images/2017/11/imperial17.pdf), Imperial College, London.


_Abstract - Recent progress in machine learning (ML) has led to an ability to solve difficult problems with unprecedented accuracy. Unfortunately, implementations using conventional software-based technologies such as central processing units, digital signal processors and graphics processing units suffer from high latency and high power consumption. In this talk, it will be explained how we have been applying Exploration, Parallelism, Integration and Customisation (EPIC) design techniques to FPGA implementations of online kernel methods, low-precision convolutional neural networks, and real-time processing of radio-frequency signals._



 	
  * 21 October 2017, Keynote Speech, [FPGA-based Machine Learning for Electronic Measurement and Instruments](/assets/images/2017/10/FPGAMLforEMI17.pdf), IEEE International Conference on Electronic Measurement and Instruments (ICEMI) Conference, Yangzhou, China




# 2015





 	
  * 

 	
    * 25 May, 2015, Lecture, [Integration of Nanoscale Structures with Electronics](/assets/images/2017/10/nanoelec-ainst17.pdf), Australian Institute for Nanoscale Science and Technology Accelerator Seminar, The University of Sydney






 	
  * 27 November, 2015, Lecture, [Architectures for the FPGA Implementation of Online Kernel Methods](/assets/images/2017/10/ml-xilinx15.pdf), Xilinx Research Labs, Dublin, Ireland




# 2014





 	
  * 1 July, 2014, Lecture, [MCALIB - Measuring Sensitivity to Rounding Error using Monte Carlo Programming](/assets/images/2017/10/mca-msra14.pdf), Microsoft Research Asia, Beijing, China




# 2013





 	
  * 25 October, 2013, Lecture, [Computer Engineering Lab](/assets/images/2017/10/cel-nicta13.pdf), National ICT Australia (NICTA), Sydney




# 2007





 	
  * 14 December, 2007, Lecture, [10 Sonnets Concerning Field Programmable Technology](/assets/images/2017/10/fpt07-whatsexciting.pdf), International Conference on Field Programmable Technology Special Session on What is Exciting about Field Programmable Technology


