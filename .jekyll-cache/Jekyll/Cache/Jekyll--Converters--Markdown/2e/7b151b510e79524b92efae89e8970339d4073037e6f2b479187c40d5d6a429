I")R<h2 id="12-may-2021-elec3607-embedded-systems-lecture">12 May 2021, ELEC3607 Embedded Systems Lecture</h2>
<p><a href="assets/talks/elec3607-sdr21.pdf">A Simple SDR</a>, 
University of Sydney</p>

<p>A simple software defined radio receiver for 40m.</p>

<h2 id="4-sep-2020-invited-lecture">4 Sep 2020, Invited Lecture</h2>
<p><a href="assets/talks/amc-rc4ml20.pdf">A Fully Parallel DNN Implementation and its Application to Automatic Modulation Classification</a>, 
<a href="https://www.fpl2020.org/">30th International Conference on Field-Programmable Logic and Applications</a>, 4th Workshop on Reconfigurable Computing for Machine Learning – RC4ML’2020</p>

<p>The high computational complexity of deep neural networks (DNNs) has led to strong interest in exploring low precision as a way to improve implementations. Unfortunately, very low precision activations and weights can have a significant impact on accuracy. This work demonstrates an efficient DNN which uses throughput matching where higher precision on certain layers can be used to recover this accuracy. This is applied to the domain of automatic modulation classification for radio signals leveraging the RF capabilities offered by the Xilinx ZCU111 RFSoC platform. The implemented networks achieve high-speed real-time performance with a classification latency of ≈8µs, and an operational throughput of 488k classifications per second. On the open-source RadioML dataset, we demonstrate how to recover 4.3% in accuracy using our technique.</p>

<h2 id="9-july-2020-lecture">9 July 2020, Lecture</h2>
<p><a href="assets/talks/rcintro20-nbn.pdf">FPGAs - EPIC Benefits</a>, NBN CTO Office</p>

<p>A field-programmable gate array (FPGA) is an integrated circuit in which the logic and interconnections can be defined by downloading a bitstream to its memory. Since their introduction in the 1980’s, these devices have dramatically increased in capacity and functionality, with the recent Xilinx ZU29DR FPGA integrating a 1.5 GHz Quad-core ARM Cortex-A53 processor; Dual-core ARM Cortex-R5; 16x 14-bit, 6.6 GSPS digital-to-analog converters; 16x 12-bit, 2.1 GSPS analog-to-digital converters; 4K DSP slices and 930K system logic gates on a single device. This presentation will first provide an overview of FPGA architectures and describe their Energy, Parallelism, Integration and Customisation (EPIC) benefits over other technologies such as microprocessors, digital signal processors and graphics processing units (GPUs). This will be followed with examples of interesting FPGA-based computing applications, including my own research in the Computer Engineering Lab at the University of Sydney. </p>

<h2 id="1-june-2020-lecture-stephen-tridgell">1 June 2020, Lecture (Stephen Tridgell)</h2>
<p><a href="http://phwl.org/2020/rtamc/">Real-time automatic modulation classification using RFSoC</a>, 27th Reconfigurable Architectures Workshop (RAW)</p>

<p>The computational complexity of deep learning has
led to research efforts to reduce the computation required.
The use of low precision is particularly effective on FPGAs as
they are not restricted to byte addressable operations. Very low
precision activations and weights can have a significant impact
on the accuracy however. This work demonstrates by exploiting
throughput matching that higher precision on certain layers
can be used to recover this accuracy. This is applied to the
domain of automatic modulation classification for radio signals
leveraging the RF capabilities offered by the Xilinx ZCU111
RFSoC platform. The implemented networks achieve high-speed
real-time performance with a classification latency of ≈8µs, and
an operational throughput of 488k classifications per second.
On the open-source RadioML dataset, we demonstrate how to
recover 4.3% in accuracy with the same hardware usage with
our technique.</p>

<h2 id="24-mar-2020-lecture">24 Mar 2020, Lecture</h2>
<p><a href="/assets/talks/LUXOR-usyd-fpga20.pdf">LUXOR: An FPGA Logic Cell Architecture for Efficient Compressor Tree Implementations</a>, 28th ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA), Monterey</p>

<p>We propose two tiers of modiications to FPGA logic cell architecture
to deliver a variety of performance and utilization beneits with
only minor area overheads. In the first tier, we augment existing
commercial logic cell datapaths with a 6-input XOR gate in order
to improve the expressiveness of each element, while maintaining
backward compatibility. This new architecture is vendor-agnostic,
and we refer to it as LUXOR. We also consider a secondary tier of
vendor-speciic modifications to both Xilinx and Intel FPGAs, which
we refer to as X-LUXOR+ and I-LUXOR+ respectively. We demonstrate
that compressor tree synthesis using generalized parallel
counters (GPCs) is further improved with the proposed modiications.
Using both the Intel adaptive logic module and the Xilinx
slice at the 65nm technology node for a comparative study, it is
shown that the silicon area overhead is less than 0.5% for LUXOR
and 5-6% for LUXOR+, while the delay increments are 1-6% and
3-9% respectively. We demonstrate that LUXOR can deliver an average
reduction of 13-19% in logic utilization on micro-benchmarks
from a variety of domains. BNN benchmarks benefit the most with
an average reduction of 37-47% in logic utilization, which is due
to the highly-efficient mapping of the XnorPopcount operation on
our proposed LUXOR+ logic cells.</p>

<h2 id="19-feb-2020-lecture">19 Feb 2020, Lecture</h2>
<p><a href="/assets/talks/lpcnn-xsj20.pdf">Low-precision CNN Inference and Training for Edge Applications on FPGAs</a>, Xilinx Research Labs, San Jose</p>

<p>Much of the focus on FPGA-based implementation of convolutional neural networks (CNNs) has targeted server-based applications for which the primary benchmark set is Imagenet.
In this talk, we discuss our research on FPGA-based inference and training for small convolutional neural networks, intended for edge applications such as radio frequency machine learning (RFML).</p>

<p>For inference, an open-source, ternary neural network generator which produces fully unrolled designs is presented. A VGG-style network implemented with this approach can achieve 90.9% accuracy for CIFAR10. On an Ultrascale+ VU9P, it achieves 122 k frames per second, with a latency of 29 μs. When applied to radio frequency modulation classification on a Xilinx ZCU111 board, we achieve 488 k classifications per second with a latency of 8 μs.</p>

<p>We also present a mixed-precision training accelerator, where most of the computations are processed using 8-bit block floating point. The work includes modifications to the stochastic weight averaging low-precision (SWALP) algorithm to achieve a 0.5% accuracy improvement for the MNIST and CIFAR10 benchmarks, bringing it within 0.1% of floating-point.</p>

<h2 id="26th-november-2019-lecture">26th November 2019, Lecture</h2>
<p><a href="/assets/talks/ml-multipliers-imperial19.pdf">Multipliers for FPGA Machine Learning Applications</a>, Imperial College</p>

<p>FPGA implementations of machine learning algorithms have received considerable interest, and multipliers are a key component for their efficient implementation. In this talk, four examples of our recent research efforts in customisation of FPGA architecture and designs using different approaches to multiplication be described. As the multipliers become more specialised, higher performance gains are possible.</p>

<p>First we describe a two-speed multiplier (TSM) which offers improved average clock rate over a conventional multiplier by carefully considering the delay of two critical paths in a serial-parallel modified radix-4 Booth multiplier. Secondly, our joint research with Fudan University on a flexible precision, run-time decomposable DSP architecture for CNN implementations which optimises Precision, Interconnect, and Reuse (PIR-DSP) is presented. The resulting design offers a 6× improvement in multiply-accumulate operations per DSP in the 9 × 9-bit case, 12× for 4 × 4 bits, and 24× for 2 × 2 bits. Thirdly, we present AddNet which shows that reconfigurable constant coefficient multipliers (RCCMs) offer an alternative to low precision arithmetic for saving area. Finally, we describe a ternary neural network generator which can generate fully unrolled designs. The implementation of a VGG-style network achieves 90.9% accuracy and 122 k frames per second, with a latency of only 29 μs, which is the fastest CNN inference implementation reported so far on an FPGA.</p>

<h2 id="7th-november-2019-lecture">7th November 2019, Lecture</h2>
<p><a href="/assets/talks/cel-tasforum19.pdf">Computer Engineering Lab</a>, Defence Innovation Network Industry Forum, University of Technology, Sydney</p>

<p>The Computer Engineering Lab at the University of Sydney focusses on how to use FPGAs to solve demanding problems. We strive to develop novel architectures, applications and design techniques for problems combining signal processing and machine learning.</p>

<h2 id="23rd-oct-2019-lunchtime-tutorial">23rd Oct 2019, Lunchtime Tutorial</h2>
<p><a href="/assets/talks/mr-tutorial19.pdf">Python Map-Reduce tutorial</a>, Computer Engineering Lab</p>

<p>This Python tutorial introduces the map-reduce programming model.
Map-reduce can be used to simplify code as well as to parallelise
computer programs. It is commonly used not only in Python programs, but
also in reconfigurable computing and large-scale computing.</p>

<h2 id="30th-sep-2019-lecture">30th Sep 2019, Lecture</h2>
<p><a href="/assets/talks/ml-multipliers19.pdf">Multipliers for FPGA Machine Learning Applications</a>, Fudan University</p>

<p>FPGA implementations of machine learning algorithms have received considerable interest, and multipliers are a key component for their efficient implementation. In this talk, four examples of our recent research efforts in customisation of FPGA architecture and designs using different approaches to multiplication be described. As the multipliers become more specialised, higher performance gains are possible.</p>

<p>First we describe a two-speed multiplier (TSM) which offers improved average clock rate over a conventional multiplier by carefully considering the delay of two critical paths in a serial-parallel modified radix-4 Booth multiplier. Secondly, our joint research with Fudan University on a flexible precision, run-time decomposable DSP architecture for CNN implementations which optimises Precision, Interconnect, and Reuse (PIR-DSP) is presented. The resulting design offers a 6× improvement in multiply-accumulate operations per DSP in the 9 × 9-bit case, 12× for 4 × 4 bits, and 24× for 2 × 2 bits. Thirdly, we present AddNet which shows that reconfigurable constant coefficient multipliers (RCCMs) offer an alternative to low precision arithmetic for saving area. Finally, we describe a ternary neural network generator which can generate fully unrolled designs. The implementation of a VGG-style network achieves 90.9% accuracy and 122 k frames per second, with a latency of only 29 μs, which is the fastest CNN inference implementation reported so far on an FPGA.</p>

<h2 id="10th-may-2019-lecture">10th May 2019, Lecture</h2>
<p><a href="/assets/images/2019/05/efficientML19.pdf">Efficient FPGA implementations of Machine Learning Algorithms</a>, San Jose State University</p>

<p>FPGA implementations of machine learning algorithms have been shown to be extremely efficient when the problem fits entirely on the FPGA but it remains a challenge to scale to problems of interest to industry. In this talk, our recent research on how to increase the capacity of existing approaches will be described.</p>

<p>In the first part of the talk we will describe an implementation of the Fastfood algorithm for scaling up online kernel methods. By utilizing the theory of random projections, problems with 1000x larger input dimension and dictionary size can be solved. A systolic implementation that operates at 500 MHz clock frequency was achieved. The next part of the talk describes how the resource requirements  in convolutional neural networks can be reduced using symmetric quantisation (SYQ), which achieves state of the art accuracy for binary and ternary weights. Finally, our work on new DSP-block architectures within FPGAs, optimised for energy-efficient embedded machine learning will be presented.</p>

<h2 id="7th-december-2018-lecture">7th December 2018, Lecture</h2>
<p><a href="/assets/images/2018/12/largescaleML18.pdf">Large Scale FPGA Implementations of Machine Learning Algorithms</a>, Tokyo Institute of Technology</p>

<p><em>FPGA implementations of machine learning algorithms have been shown to be extremely efficient when the problem fits entirely on the FPGA but it remains a challenge to scale to problems of interest to industry. In this talk, our recent research on how to increase the capacity of existing approaches will be described.</em></p>

<p><em>In the first part of the talk we will describe an implementation of the Fastfood algorithm for scaling up online kernel methods. By utilizing the theory of random projections, problems with 1000x larger input dimension and dictionary size can be solved. A systolic implementation that operates at 500 MHz clock frequency was achieved. The next part of the talk describes how the resource requirements  in convolutional neural networks can be reduced using symmetric quantisation (SYQ), which achieves state of the art accuracy for binary and ternary weights. Finally, our work on an open-source matrix multiplication library on the Xeon+FPGA platform is described. This allows a number of different precisions down to binary, and additional support for deep learning applications is provided. Performance approaching the maximum of 14 Xeon cores and an Arria 10 FPGA was achieved.</em></p>

<h2 id="26-november-2018-lecture">26 November 2018, Lecture</h2>
<p><a href="/assets/images/2018/11/fixedml-imperial18.pdf">Fixed-point FPGA implementations of Machine Learning Algorithms</a>, Department of Computing, Imperial College</p>

<p><em>Abstract: FPGA implementations of machine learning algorithms have been shown to be extremely efficient, particularly when fixed-point arithmetic can be utilised to simplify the datapath of a hardware implementation. In this talk, our recent research on how accuracy and performance of fixed-point implementations of neural networks can be improved will be presented.</em></p>

<p><em>In the first part of the talk we describe how the resource requirements  in convolutional neural networks can be reduced using symmetric quantisation (SYQ), which achieves state of the art accuracy for binary and ternary weights. Next, a variant of the serial-parallel modified radix-4 Booth multiplier that adds only the non-zero Booth encodings and skips over the zero operations, achieving a 1.4x Area-Time improvement over the standard parallel Booth multiplier is presented. Finally, we describe a system implementation of a long short-term memory (LSTM) based spectral prediction accelerator for physical layer radio frequency applications, with latency below 5 microseconds.</em></p>

<h2 id="29-october-2018-lecture">29 October 2018, Lecture</h2>
<p><a href="/assets/images/2018/10/lstmslides-milcom18.pdf">Long short-term memory for radio frequency spectral prediction and its real-time FPGA implementation</a>, MILCOM Los Angeles</p>

<h2 id="26-september-2018-lecture">26 September 2018, Lecture</h2>
<p><a href="/assets/images/2018/09/TeachingandResearchatUSyd-HIT18.pdf">Teaching and Research at the University of Sydney</a>, Harbin Institute of Technology</p>

<h2 id="26-september-2018-lecture-1">26 September 2018, Lecture</h2>
<p><a href="/harbin-intitute-of-technology-reconfigurable-computing-course-2018/">Reconfigurable Computing</a>, Harbin Institute of Technology Short Graduate Course</p>

<h2 id="25-september-2018-lecture">25 September 2018, Lecture</h2>
<p><a href="/assets/images/2018/09/DLT-presentation-60-min.pdf">Discover your future in engineering and technology</a>, D &amp; LT Consultants, Harbin</p>

<h2 id="24-july-2018-lecture">24 July 2018, Lecture</h2>
<p><a href="/assets/images/2018/11/quantisation-papaa18.pdf">Quantisation</a>, <a href="http://cscpapaa.eee.hku.hk/">Performance-Aware Programming with Application Accelerators (PAPAA)</a>, Hong Kong</p>

<h2 id="30-november-2017-lecture">30 November 2017, Lecture</h2>
<p><a href="/assets/images/2017/11/imperial17.pdf">FPGA-based Implementations of Machine Learning Algorithms and the EPIC Approach</a>, Imperial College, London.</p>

<p>Abstract - Recent progress in machine learning (ML) has led to an ability to solve difficult problems with unprecedented accuracy. Unfortunately, implementations using conventional software-based technologies such as central processing units, digital signal processors and graphics processing units suffer from high latency and high power consumption. In this talk, it will be explained how we have been applying Exploration, Parallelism, Integration and Customisation (EPIC) design techniques to FPGA implementations of online kernel methods, low-precision convolutional neural networks, and real-time processing of radio-frequency signals._</p>

<h2 id="21-october-2017keynote-speech">21 October 2017, Keynote Speech</h2>
<p><a href="/assets/images/2017/10/FPGAMLforEMI17.pdf">FPGA-based Machine Learning for Electronic Measurement and Instruments</a>, IEEE International Conference on Electronic Measurement and Instruments (ICEMI) Conference, Yangzhou, China</p>

<h2 id="24-october-2016-lecture">24 October 2016, Lecture</h2>
<p><a href="https://www.microsoft.com/en-us/research/video/architectures-fpga-implementation-online-kernel-methods/">Architectures for the FPGA Implementation of Online Kernel Methods (link to video)</a>, 
Microsoft Research Talks, Redmond</p>

<p>In machine learning, traditional linear prediction techniques are well understood and methods for their efficient solution have been developed. Many real-world applications are better modelled using non-linear techniques, which often have high computational requirements. Kernel methods utilise linear methods in a non-linear feature space and combine the advantages of both. Commonly used kernel methods include the support vector machine (SVM), Gaussian processes and regularisation networks. These are batch-based, and a global optimisation is conducted over all input exemplars to create a model. In contrast, online methods, such as the kernel recursive least squares (KRLS) algorithm, update the state in a recursive and incremental fashion upon receiving a new exemplar. Although not as extensively studied as batch methods, online approaches are advantageous when throughput and latency are critical. In this talk I will describe efforts in the Computer Engineering Laboratory to produce high-performance FPGA-based implementations of online kernel methods. These have included: (1) a microcoded vector processor optimised for kernel methods; (2) a fully pipelined implementation of kernel normalised least mean squares which achieves 160 GFLOPS; (3) an implementation of Naive Online regularised Risk Minimization Algorithm (NORMA) which uses “braiding” to resolve data hazards and reduce latency by an order of magnitude; and (4) a distributed kernel recursive least squares algorithm which constructs a compact model while enabling massive parallelism.</p>

<h2 id="25-may-2015-lecture">25 May 2015, Lecture</h2>
<p><a href="/assets/images/2017/10/nanoelec-ainst17.pdf">Integration of Nanoscale Structures with Electronics</a>, Australian Institute for Nanoscale Science and Technology Accelerator Seminar, The University of Sydney</p>

<h2 id="27-november-2015-lecture">27 November 2015, Lecture</h2>
<p><a href="/assets/images/2017/10/ml-xilinx15.pdf">Architectures for the FPGA Implementation of Online Kernel Methods</a>, Xilinx Research Labs, Dublin, Ireland</p>

<h2 id="1-july-2014-lecture">1 July 2014, Lecture</h2>
<p><a href="/assets/images/2017/10/mca-msra14.pdf">MCALIB - Measuring Sensitivity to Rounding Error using Monte Carlo Programming</a>, Microsoft Research Asia, Beijing, China</p>

<h2 id="25-october-2013-lecture">25 October 2013, Lecture</h2>
<p><a href="/assets/images/2017/10/cel-nicta13.pdf">Computer Engineering Lab</a>, National ICT Australia (NICTA), Sydney</p>

<h2 id="15-may-2009-lunchtime-talk">15 May 2009, Lunchtime talk</h2>
<p><a href="http://phwl.org/2009/life-the-universe-and-fishing/">Lunchtime research talk</a>, Chinese University of Hong Kong, Hong Kong</p>

<h2 id="14-december-2007-lecture">14 December 2007, Lecture</h2>
<p><a href="/assets/images/2017/10/fpt07-whatsexciting.pdf">10 Sonnets Concerning Field Programmable Technology</a>, International Conference on Field Programmable Technology Special Session on What is Exciting about Field Programmable Technology</p>

:ET